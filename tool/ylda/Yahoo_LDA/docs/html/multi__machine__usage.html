<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Y!LDA: Multi-Machine Setup</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.6.3 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div>
<div class="contents">


<h1><a class="anchor" id="multi_machine_usage">Multi-Machine Setup </a></h1><p>Please take a look at <a class="el" href="single__machine__usage.html">Single Machine Usage</a> for information on running individual commands. Here we give you ways to run those individual commands on multiple machines. So, we are not repeating the details on the individual commands. </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli"><b>Using Hadoop</b> </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">Run Y!LDA on the corpus: </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">Assuming you have a homogenous setup, install Y!LDA on one machine. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Run make jar to create LDALibs.jar file with all the required libraries and binaries </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Copy LDALibs.jar to HDFS </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Copy Formatter.sh LDA.sh functions.sh runLDA.sh to gateway. A gateway is any machine with access to the grid. Think of it as a machine from which you run your hadoop commands. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Figure out the max memory allowed per map task for your cluster and use the same in the script via the maxmem parameter. This can be done by looking at any job conf (job.xml) and searching the value of "mapred.cluster.max.map.memory.mb" property. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Run <code>runLDA.sh 1 "flags" [train|test] "queue" "organized-corpus" "output-dir" "max-mem" "number_of_topics" "number_of_iters" "full_hdfs_path_of_LDALibs.jar" "number_of_machines" ["training-output"] </code><br/>
<br/>
 For train ex. <code>runLDA.sh 1 "" train default "/user/me/organized-corpus" "/user/me/lda-output" 6144 100 100 "LDALibs.jar" 10 </code><br/>
<br/>
 For test ex. <code>runLDA.sh 1 "" test default "/user/me/organized-corpus" "/user/me/lda-output" 6144 100 100 "LDALibs.jar" 10 "/user/me/lda-output" </code>  </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">This starts 2 Map Reduce Streaming jobs. The first job does the formatting &amp; the second job starts a map-only script on each machine. This script starts the <a class="el" href="class_d_m___server.html" title="The Server class that implements the DistributedMap Ice interface.">DM_Server</a> on all the machines. Then Y!LDA is run on each machine. The input is one chunk of the corpus. The first job runs the formatter in the reducer using the supplied number_of_machines as the number of reduce tasks. So your corpus will be split into number_of_machines parts and formatted into protobuffer format files as needed by the second job. The second job uses this formatted input as its input and will also run on number_of_machines separate machines with each task working on one chunk. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">For testing, use the test flag and provide directory storing the training output. </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Output generated </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">Creates &lt;number_of_machines&gt; folders in &lt;output-dir&gt; one for each client.  </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Each of these directories hold the same output as the single machine case but from different clients.  </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">&lt;output-dir&gt;/&lt;client-id&gt;/learntopics.WARNING contains the output written to stderr by client &lt;client-id&gt; </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">&lt;output-dir&gt;/&lt;client-id&gt;/lda.docToTop.txt contains the topic proportions assigned to the documents in the portion of the corpus alloted to client &lt;client-id&gt; </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">&lt;output-dir&gt;/&lt;client-id&gt;/lda.topToWor.txt contains the salient words learned for each topic. This remains almost same across clients. So you can pick one of these as the salient words per topic for the full corpus. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">&lt;output-dir&gt;/&lt;client-id&gt;/lda.ttc.dump contains the actual model. Even this like the salient words is almost same across clients and any one can be used as the model for the full corpus. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">&lt;output-dir&gt;/global contains the dump of the global dictionary and the partitioned gobal topic counts table. These are generated in the training phase and are critical for the test option to work. </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Viewing progress </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">The stderr output of the code will be redirected into hadoop logs. So you can check the task logs from the tracking URL displayed in the output of runLDA.sh to see what is happening </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p>Failure Recovery </p>
<p>We provide a check-pointing mechanism to handle recovery from failures. The current scheme works in local mode and for distributed mode using Hadoop. The reason for this being that the distributed check-pointing uses the hdfs to store the check-points. The following is the process:  </p>
<ol>
<li>
The formatter task is run on the inputs and the formatted input is stored in a temporary location. </li>
<li>
The learntopics task is run using the temporary location as an input and the specified output as the output directory. Care is taken to start the same number of mappers as the number_of_machines for learntopics tasks. The input is a dummy directory structure with dummy directories equal to the number_of_machines parameter supplied by the user. </li>
<li>
Each learntopics task copies its portion of the formatted input by dfs copy_to_local the folder corresponding to its mapred_task_partition. </li>
<li>
Runs learntopics with the temporary directory containing the formatted input as a check-point directory. So all information needed to start learntopics from the previous check-pointed iteration is available locally and any progress made is written back to the temporary input directory. </li>
</ol>
<p>This mechanism is utilized by the scripts to detect failure cases and attempt to re-run the task again from the previous checkpoint. As learntopics is designed to check if check-point metadata is available in the working directory and use it to start-off from there a separate restart option is obviated.  </p>
<p>As a by product one gets the facility of doing incremental runs, that is, to run say 100 iterations, check the output and run the next 100 iterations if needed. The scripts detect this condition and ask you if you want to start-off from where you left or restart from the beginning. </p>
<p class="endli">The scripts are designed in such a fashion that these happen transparently to the user. This is information for developers and for cases where the recovery mechanism could not handle the failure in the specified number of attempts. Check the stderr logs to see what the reason for failure is. Most times it is due to wrong usage which results in unrecoverable aborts. If you think its because of a flaky cluster, then try increasing the number of attempts. If nothing works and you think there is a bug in the code please let us know. </p>
</li>
</ol>
</li>
</ol>
<ol start="2">
<li>
<p class="startli"></p>
<p class="endli"><b>Using SSH - Assume you have 'm' machines</b> </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">If you have a homogenous set up, install Y!LDA on one machine, run make jar and copy LDALibs.jar to all the other machines in the set up. Else install Y!LDA on all machines. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Split the corpus into 'm' parts and distribute them to the 'm' machines </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Run formatter on each split of the corpus on every machine. </p>
</li>
<li>
<p class="startli"></p>
<p>Run the Distributed_Map Server on each machine as a background process using nohup: </p>
<p class="endli"><code>nohup ./DM_Server &lt;model&gt; &lt;server_id&gt; &lt;num_clients&gt; &lt;host:port&gt; --Ice.ThreadPool.Server.SizeMax=9 &amp;</code> </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">model: an integer that represents the model. Set it to 1 for <a class="el" href="class_unigram___model.html">Unigram_Model</a> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">server_id: a number that denotes the index of this server in the list of servers that is provided to 'learntopics'. If server1 has h:p, 10.1.1.1:10000 &amp; is assigned id 0, server2 has h:p, 10.1.1.2:10000 &amp; is assigned id 1, the list of servers that is provided to 'learntopics' has to be 10.1.1.1:10000, 10.1.1.2:10000 and not the other way around. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">num_clients: a number that denotes the number of clients that will access the Distributed Map. This is usually equal to 'm'. This is used to provide a barrier implementation </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">host:port- the port and ip address on which the server must listen on </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Run Y!LDA on the corpus: </p>
<ol>
<li>
<p class="startli"></p>
<p>On every machine run </p>
<p class="endli"><code>learntopics --topics=&lt;topics&gt; --iter=&lt;iter&gt; --servers=&lt;list-of-servers&gt; --chkptdir="/tmp" --chkptinterval=10000</code> </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli">&lt;list-of-servers&gt;: The comma separated list of ip:port numbers of the servers involved in the set-up. The index of the ip:port numbers should be as per the server_id parameter used in starting the server </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">chkptdir &amp; chkptinterval: These are currently used only with the Hadoop set-up. Set chkptdir to something dummy. In order that the checkpointing code does not execute, set the chkptinterval to a very large value or some number greater than the number of iterations </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Create Global Dictionary - Run the following on server with id 0. Assuming learntopics was run in the folder /tmp/corpus </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli"><code>mkdir -p /tmp/corpus/global_dict; cd /tmp/corpus/global_dict;</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>scp server_i:/tmp/corpus/lda.dict.dump lda.dict.dump.i </code> where the variable 'i' is the same as the server_id. </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">Merge Dictionaries<br/>
 <code>Merge_Dictionaries --dictionaries=m --dumpprefix=lda.dict.dump</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>mkdir -p ../global; mkdir -p ../global/topic_counts; cp lda.dict.dump ../global/;</code>  </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Create a sharded Global Word-Topic Counts dump - Run on every machine in the set-up </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli"><code>mkdir -p /tmp/corpus/global_top_cnts; cd /tmp/corpus/global_top_cnts;</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>scp server_0:/tmp/corpus/global/lda.dict.dump lda.dict.dump.global</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>Merge_Topic_Counts --topics=&lt;topics&gt; --clientid=&lt;server-id&gt; --servers=&lt;list-of-servers&gt; --globaldictionary="lda.dict.dump.global"</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>scp lda.ttc.dump server_0:/tmp/corpus/global/topic_counts/lda.ttc.dump.$server-id</code> </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Copy the parameters dump file to global dump - Run on server_0 </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli"><code>cd /tmp/corpus; cp lda.par.dump global/topic_counts/lda.par.dump</code> </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">This completes training and the model is available on server_0:/tmp/corpus/global </p>
</li>
</ol>
</li>
<li>
<p class="startli"></p>
<p class="endli">Running Y!LDA is test mode: Run on server_0. Assuming test corpus is in /tmp/test_corpus </p>
<ol>
<li>
<p class="startli"></p>
<p class="endli"><code>cd /tmp/test_corpus;</code>  </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>cp -r ../corpus/global .</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli"><code>learntopics -teststream --dumpprefix=global/topic_counts/lda --numdumps=m --dictionary=global/lda.dict.dump --maxmemory=2048 --topics=&lt;topics&gt;</code> </p>
</li>
<li>
<p class="startli"></p>
<p class="endli">cat all your documents, in the same format that 'formatter' expects, to the above command's stdin </p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
<hr class="footer"/><address style="text-align: right;"><small>Generated on Tue Jul 19 11:45:25 2011 for Y!LDA by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.6.3 </small></address>
</body>
</html>
