
%% bare_conf.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{multirow}
\usepackage{url}
\usepackage{cite}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor syncLocalWithGlobal syncGlobalWithLocal rotateGlobal}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Abstract Parallel LDA Through Optimized Synchronous Communication Methods}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Michael Shell}
% \IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
% Georgia Institute of Technology\\
% Atlanta, Georgia 30332--0250\\
% Email: http://www.michaelshell.org/contact.html}
% \and
% \IEEEauthorblockN{Homer Simpson}
% \IEEEauthorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
% \IEEEauthorblockA{Starfleet Academy\\
% San Francisco, California 96678--2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}
\author{\IEEEauthorblockN{Bingjing Zhang, Bo Peng, Judy Qiu}
	\IEEEauthorblockA{Computer Science Department\\
		Indiana University\\
		Bloomington, IN, USA\\
		zhangbj, pengb, xqiu@indiana.edu}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
% We need to update abstract
% Petuum LDA is different from what it is described in their papers
% No key-value store is seen in the implementation. 
% the routing pattern is also close to our rtt implementation
% How do we delilver this information is the paper? 
Selecting an appropriate distributed processing framework can be difficult 
when developers build large-scale machine learning applications.
This is because all these tools provide various kinds of parallelism patterns
and suggest different communication strategies to synchronize 
local and global model data distributed among parallel nodes.
There is no clear answer to determine
which strategy might be suitable depending on the data and application.
Taking Latent Dirichlet Allocation (LDA) as an example,
contemporary implementations
often choose asynchronous communication methods
to synchronize the model data.
However, our observations shows that the asynchronous communication
still has very high overhead and the characteristics of 
the LDA training datasets encourage us to use 
optimized synchronous collective communication methods instead. 
The results show that with data parallelism only, 
our "lda-lgs" implementation can be (\%) faster compared with Yahoo!LDA.
With model parallelism,
our "lda-rtt" implementation has similar speed 
compared with Petuum LDA on a uni-gram model with 1 million words 
and 10k topics but (\%) faster on a bi-gram model with 20 million words and 500 topics.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Nowadays people attempt many different distributed processing frameworks 
to build machine learning application.
However these frameworks are very different from each other,
and knowing which one is suitable to your own application is difficult.
For example, Latent Dirichlet Allocation (LDA) \cite{1} has been implemented
on MPI \cite{2}, Hadoop \cite{3}, Spark \cite{4}, Dato \cite{5},
Parameter Server \cite{6}, and Petuum \cite{7}. 
It can also be implemented as a standalone distributed application
such as Yahoo!LDA \cite{8} and Peacock \cite{9}. 

Direct comparison between
various these implementations is impossible for two reasons.
Firstly these implementations may use different algorithms to perform LDA.
For example, Mahout LDA \cite{10} and Spark LDA \cite{11} 
use Collapsed Variational Bayes (CVB) algorithm \cite{1}. 
But this algorithm is considered not scalable
due to the high memory consumption \cite{12}.
Other implementations such as
pLDA \cite{13} on MPI, Yahoo!LDA, Dato PowerGraph LDA \cite{14},
Parameter Server LDA, Peacock and Petuum LDA \cite{15}
use Collapsed Gibbs Sampling (CGS) algorithm \cite{16}
due to its superior scalability.
Yahoo!LDA, Peacock and Petuum LDA both
use a special CGS implementation called SparseLDA \cite{17}
which can utilize the sparsity of the word-topic model matrix 
and greatly reduce the computation time of the original CGS algorithm.

\begin {table}
\caption{LDA Implementations Using CGS Algorithm}
\label{tab:1}
\centering
\begin{tabular}{|m{1.5cm}|m{2.6cm}|m{1.6cm}|m{1.0cm}|}
	\hline
	App. Name&Algorithm&Parallelism&Comm.\\
	\hline
	pLDA&CGS (sample by docs)&D. P.&allreduce (sync)\\
	\hline
	Dato&CGS (sample by doc-word edge)&D. P.&GAS (sync)\\
	\hline
	Yahoo!LDA&
	CGS (SparseLDA \& sample by docs)
	&D. P.&client-server (async)\\
	\hline
	Peacock&CGS (SparseLDA \& sample by words)
	&D. P. (M. P. in local)&client-server (async)\\
	\hline
	Parameter Server 
	&CGS (combined with other methods)
	&D. P.&client-server (async)\\
	\hline
	Petuum 0.93
	&CGS (SparseLDA \& sample by docs)
	&D. P.&client-server (async)\\
	\hline
	Petuum 1.1
	&CGS (SparseLDA \& sample by words)
	&M. P. (include D. P.)&ring topology (async)\\\hline
\end{tabular}
\end{table}
Secondly these implementations use different synchronization strategies
(see Table. \ref{tab:1}). 
Among the LDA implementations based on the CGS algorithm,
there are two methods of parallelism.
One is called data parallelism. Here the training data is split
between parallel workers. Each worker has a local model for the local sampling.
And the local model is updated through the local model and global model 
synchronization. The synchronization can be done synchronously or asynchronously.
pLDA uses allreduce operation \cite{18}
to synchronize local updates on the global model.
Dato PowerGraph LDA uses synchronous gather-apply-scatter (GAS) model \cite{19}. 
But popular implementations such as
Yahoo!LDA, Parameter Server LDA and Petuum LDA \cite{20} 
which have shown good scalability use client-server architecture 
with asynchronous communication methods
to exchange the local data and the global model.
Another method is model parallelism. In addition to using data parallelism,
the global model data is split between parallel workers. 
These workers exchange model data partitions between each other
and finally update all the partitions.
Model parallelism can significantly reduce the model convergence time. 
Petuum has another LDA implementation which uses model parallelism \cite{21}.
In this implementation, the workers form a ring topology.
When one worker finish processing the model partition it owns,
it sends the data to its neighbor on the right
and continue sampling data received from the left-hand neighbor.
The local sampling goes through each word and sample related documents
but not traverse each document and sample related words.
Once a word is sampled, it is sent out immediately.
In this way, the local computation and the communication are done
in an asynchronous way and are highly overlapped.

Some implementations use synchronous communication strategy,
while others use asynchronous communication strategy (see Fig. \ref{fig:1}).
Under different parallelism patterns,
it is unclear which strategy is suitable for LDA application.
Communication is important not only 
because a faster method can reduce the resulting overhead,
but also because it can speed up the model converge rate
and result in shorten computation time per iteration.
The related work show that asynchronous communication is getting popular.
In data parallelism, Yahoo!LDA, Parameter Server and Petuum all
use asynchronous communication to exchange the local model and the global model.
Here ''asynchronous'' means 
local computation continues without waiting for 
the completion of the global model update from all the parallel workers.
In model parallelism, each worker needs to send/receive all
the model partitions in one iteration, so the communication has to be synchronous.
But in local, once a word is sampled, its model content is sent
and the computation still continues sampling the next word
without waiting for ``send/receive''.
``Asynchronous'' occurs between the local computation and communication.
\begin{figure}
	\centering
	\begin{tabular}{c}
		\subfloat[][Data Parallelism \& Model Parallelism]{
			\includegraphics[width=8cm,height=6cm]{figs//arch//arch.pdf}
			\label{fig_a} 
		}
		\\
		\subfloat[][Asynchronous Communication \& Synchronous Communication]{
			\includegraphics[width=8cm,height=6cm]{figs//arch//sync.pdf}
			\label{fig_b}
		} 
	\end{tabular}
	\caption{Parallelism Patterns and Synchronization Strategies in LDA}
	\label{fig:1}
\end{figure}

We summarize the advantages of using asynchronous communication methods below:
1) Avoid the overhead of global waiting between multiple parallel workers. 
2) Avoid the overhead of local waiting between computation threads
and communication threads through overlapping. 
In data parallelism,
the idea behind of these design is:
though global model (words' topic counts) is huge,
since it is distributed sparsely,
each worker only holds a small local model data 
and the related data amount for communication is low. 
In model parallelism,
the global model is divided into $N$ splits ($N$ is the number of workers),
each global model partition is considered small
and as a result the related data amount during sending is low. 
In contrast, the synchronous communication methods
are considered inefficient 
due to the global waiting between parallel workers 
and the local waiting between the computation/communication threads.
Besides of these problems, there are other issues 
with the implementations for synchronous communication methods.
pLDA which uses allreduce operation does not exploit the model sparsity
and requires all the workers to receive all the global model data.
Dato PowerGraph does consider the model sparsity
but its graph based GAS abstractions
are inefficient for expressing LDA algorithm.    

After studying the characteristics of LDA data, 
we identify the count of each word in a collection of documents
is under the power-law distribution.
As are result, when the data parallelism is used,
many words in the global model could still show in all the workers' local models.
Thus when the client-server architecture and asynchronous communication methods are used,
there are in fact many "one-to-all" communication patterns between servers and clients.
Similarly in model parallelism, the total amount of model data 
sent and received on each worker during one iteration
is the same as the size of the whole global model.
As the size of model data expands, each worker still needs
to handle much data during sending and receiving.
These observations inspired us to use 
routing optimized collective communication operations
to improve the synchronization speed in LDA.
 
So what is the difference between our work and previous research works
using synchronous communication methods?
"allreduce" operation used in pLDA is optimized in routing
but fails to explore the sparsity of the model distribution. 
Dato PowerGraph considers the sparsity but not routing optimization.
Our method combines both advantages from exploiting model distribution sparsity
and routing optimization at the same time. 
Furthermore, we overlap the synchronization points of the computation threads and the communication threads
to reduce the overhead of the global/local waiting.   

These ideas are implemented in Harp \cite{22}, a collective communication library on Hadoop.
Harp has already integrated several collective communication patterns from
different distributed processing frameworks in a unified abstraction. These patterns are: 
1) ``broadcast'', ``reduce'', ``allgather'', ``allreduce'' from MPI \cite{19}.
2) ``regroup'' operation from MapReduce \cite{23}.
3) graph-like communication from Pregel \cite{24} and Giraph \cite{25}. 
According to our analysis above, all these current patterns still cannot abstract
either the local/global model synchronization in the data parallelism
or the model rotation in the model parallelism in LDA algorithm.
As such we abstract another three communication patterns called "rotateGlobal",
"syncLocalWithGlobal" and "syncGlobalWithLocal" in which our new ideas are embedded.
Later we will explain how these new patterns 
are general enough so that they can replace "regroup"
or graph based communications used in many applications.

We implement LDA algorithm with new abstractions in Harp. 
One is based on paired "syncLocalWithGlobal" and "syncGlobalWithLocal"
to perform data parallelism.
Another is based on "rotateGlobal" operation to do model parallelism.
Compared with other LDA implementations which also use similar parallelism patterns,
our innovation involves abandoning asynchronous communication methods
and adopting optimized synchronous collective communication.
We compared our implementation with Yahoo!LDA and Petuum on two different datasets
with a model of 10 billion parameters
(1 million words and 10 thousand topics)
The result shows that...
We also tested a bi-gram model with up to 20 million words and 500 topics per word in the model.


In the following sections,
Section 2 gives the cost model between model data size and the communication.
Section 3 describes the synchronous communication methods.
Section 4 talks about how we use the abstractions to build Harp-LDA.
Section 5 explains the performance results of our implementation.
Section 6 talks about the related work on parallel LDA.
Section 7 gives our final conclusion.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
\section{Cost Model}
% \input{cost_model.tex}
%
%
%
\section{Synchronous Communication Methods}
\subsection{Existing Collective Communication Abstraction}
% Introduce Harp
Much research work has shown that collective communication operations
are indispensable in iteration based machine learning algorithms.
\cite{26} points out that 
many machine learning algorithms can be implemented in MapReduce systems.
The principle behind this conclusion
is that the computation of each iteration in the algorithm 
is dependent on the update of the global model data
which is done through the synchronization of the local model data on each worker. 
However MapReduce systems only provides a fixed communication pattern:
disk-based shuffling whose performance is poor on
iteration based machine learning algorithms. 
For Harp we provide a separate collective communication abstraction layer 
which provides a set of data abstractions
and related collective communication operation abstractions.

% Why we need collective communication
% Why the traditional collective communication operations don't work
From the previous sections,
we already know that in data parallelism
the local model data required by the computation
could cover a large part of the whole global data. 
Under this situation, ''one-to-all'' communication pattern still play an important role 
in the whole communication so that it is possible to optimize the communication performance 
with collective communication operations rather than just conducting point-to-point communication.

In addition, model parallelism requires that
each model partition rotate on all the workers. As a result,
this communication happens synchronously per iteration. 
As such it is possible to abstract model rotation as
a collective communication operation where each worker can maximize using
the bandwidth between itself and the neighbor nodes during shifting
the model partitions.
\subsection{The Abstraction Of Global/Local Data Synchronization}
Considering the sparsity of the model data distribution on workers 
and collective communication optimization together,  
beyond the existing collective communication abstractions in Harp,
we add another two data abstractions
and related new collective communication operations.

% Describe the table for local model and the table for global model.
First let us define two types of data abstractions. 
One is called the global table and another is the local table.
The concept ``table'' has been shown in previous
Harp collective communication abstraction \cite{22}.
Each table can contain one or more partitions
and the tables defined on different workers 
are associated with each other in order to manage a distributed dataset.
In global tables, a partition has a unique ID 
and represents a part of the whole distributed dataset.
But in local tables, partitions on different workers
can share the same partition ID.
Each of these partitions is considered 
local version of a part of the distributed dataset.  

% Describe "syncLocalWithGlobal", "syncGlobalWithLocal", and "rotateGlobal" operation.
We define three communication operations on global tables and local tables.
The first two are paired operations. One is ``syncGlobalWithLocal'', which means
using the data in the local tables to synchronize the data in the global tables.
This operation will re-distribute the partitions from local tables to the global table.
If a partition has one or more versions in local tables of different workers,
these local versions will be combined into one partition. 

Another is ``syncLocalWithGlobal'', 
which means using the data in the global tables
to synchronize the local tables.
Based on the needs of partitions in the local tables,
this operation will re-distribute the partitions
in the global tables to the local tables. 
If one partition is required by all the workers,
then this is a ``one-to-all'' communication pattern 
and can be optimized through "broadcast" algorithms.

The third operation is called ``rotateGlobal''.
This operation will consider workers in a ring topology and 
shift the partitions in the global table
owned by one worker to the right neighbor worker
then receive the partitions from the left neighbor.
When the operation is done,
the contents of the distributed dataset in global tables won't change 
but each worker will hold a different set of partitions. 
Because a worker only talks to each of its neighbors. 
``rotateGlobal'' can transmit global data in parallel
without any network conflicts. 
\subsection{The Applicability of Synchronous Communication Methods}
It is obvious that both ``syncGlobalWithLocal'' and ``syncLocalWithGlobal''
are abstracted from the communication pattern in data parallelism
and ``rotateGlobal'' is abstracted from model parallelism.
But these operations are not limited to the communication patterns in parallel LDA,
Instead they can be applied to many other machine learning algorithms
with large model data.

We can draw a matrix to describe
each worker's requirement on global model data
in the parallel computation.
In this matrix, each row represents a worker,
each column represents a global data partition
and each element shows the requirement of
the data partition in the local computation.
Based on the density of this computation relation matrix,
we can choose proper operations in different machine learning applications.
If the local computation needs all the global model data to compute,
we suggest using ``rotateGlobal'' operation. 
Taking k-means clustering as an example, the global model data are the centroids 
and the local computation needs all the centroids data.
Thus ``rotateGlobal'' allows each worker
to access all the centroids data efficiently.

If the computation relation matrix is sparse,
using ``syncGlobalWithLocal'' and ``syncLocalWithGlobal''
these two paired operations is a superior solution.
For example, in many graph algorithms such as PageRank,
the original implementations use ''allgather'' operation in MPI, 
``send messages from vertices to vertice'' operation in Giraph,
or GAS model in Dato PowerGraph.
Now all of them can be replaced with these two operations. 
The global model data are vertices' page rank values and counts of out-edges.
The local computation goes through each edge
and gets the partial result of the new page-rank values.
We can use ``syncGlobalWithLocal'' to update global page-rank values
with local partial page-rank values. 
In the next iteration, we can use ``syncLocalWithGlobal''
to fetch the new global page rank values to each local computation.

In summary, many iteration based machine learning algorithms
can be expressed with these operations. 
As the model data increases, by exploiting routing optimization 
and the sparsity of the model data distribution,
these operations can benefit many applications.
%
%
\section{Harp-LDA Implementation}
% The relationship between local computation and the global data
Our LDA implementation uses the CGS algorithm
with the new collective communication abstractions.
There are two implementations.
One is based on the data parallelism. Similar to Yahoo!LDA,
the local computation uses SparseLDA algorithm and
does sampling document by document.
Another is based on the model parallelism. Similar to Petuum LDA,
the local computation uses SparseLDA algorithm and handles sampling word by word. 

\subsection{Partition Training Data And Model Data}
For the training data,
we split the data into files each of which contains a similar number of tokens 
and at the same time do not break any document.
During the execution, each worker owns an even part of all these files. 
Because the documents may vary in the number of tokens 
and some may be much longer than the rest,
partitioning based on tokens can balance the computation load on each worker.

% Local/global model partitioning
In LDA, there are two kinds of model data:
one includes all the documents' topic count distribution
and another has all the words' topic count distribution. 
Because the documents have already been partitioned to each worker,
the documents' topic count distribution can be stored locally.
As a result, the model data required for synchronization 
is the word's topic distribution and this is what referred as
``global model''.
 
Because words with high frequency
can dominate the computation and the communication, 
we partition the global model based on
the frequency of the words in the training dataset. 
During the preprocessing of the training data,
each word is given an ID based on their frequency. 
The word ID starts from 0 and then proceeds to 1, 2 and so on. 
The lower the occurrence of the word, the higher the ID value.
Then we partition the words' topic counts using range-based partitioning. 
Assuming each partition contains $m$ words, 
the first partition Partition 0 contains words with ID from 0 to $m-1$, 
and then Partition 1 contains words with ID from $m$ to $2m-1$ and so on. 
As a result, the partitions with low IDs
contain the words with the highest frequency. 
The initial global model is generated and distributed on all the workers 
through the synchronous communication methods designed.
Mapping between partition IDs and worker IDs
is calculated through the modulo operation.
In this way, each worker contains
a number of words whose frequencies rank from high to low.
Assuming there is a worker with ID $w$ in total $N$ workers,
the partitions contained on this worker are
Partition $w$, Partition $w+N$, Partition $w+2N$ and so on. 
\subsection{Parallelism Design}
% Communication methods
In Harp-LDA, we distribute each worker to a compute node. 
Each worker loads a portion of training data into memory
and repeat the sampling process in iterations. 
In global model initialization, 
every word in the training documents is randomly assigned to a topic.
Then ``syncGlobalWithLocal'' operation is used to combine the local models
and create a distributed global model across all the workers
(an alternative way to complete the initialization is through ``rotateGlobal'' method).
During iterations of the sampling,
we use two different approaches to update the global model 
which results in two implementations. 
One follows the data parallelism,
using paired ``syncGlobalWithLocal'' and ``syncLocalWithGlobal'' operations.
We name it ``lda-lgs''.
Another follows the model parallelism, and uses ``rotateGlobal" operation.
We name it ``lda-rtt'' (See Fig. \ref{fig:2}).
\begin{figure}
	\centering
	\includegraphics[width=8.8cm,height=6.6cm]{figs//arch//impl.pdf}
	\caption{Parallelism Design}
	\label{fig:2}
\end{figure}

In ``lda-lgs'',  
``syncLocalWithGlobal'' operation is first used
to download the model data needed for sampling
from the global table to the local table. 
Then the worker updates the local model 
and generates another table to track the difference generated on the local model.
Once the sampling is done, 
``syncGlobalWithLocal'' operation 
is used to update the global model with the local delta recorded. 

In ``lda-rtt'', 
each worker will first do sampling
with the global model partitions owned by itself
and update them directly. 
Then it will call ``rotateGlobal'' operation: 
send the updated model data to the right neighbor,
and receive the model partitions from the left neighbor.
Once all partitions of the global model are received and sampled,
the local sampling of one iteration completes.

Other researchers have shown that the model parallelism can brings
faster convergence speed than the data parallelism \cite{21}.
So the differences between communication methods in two implementations
not only changes the communication speed,
but also change the model convergence speed
and further affect the overall performance. 
This will be discussed later in the experiments.
%
\subsection{Overlap Communication with Computation}
Synchronous communication methods are often criticized that
the synchronization point generates lots of overhead 
and make other workers wait for the completion of the synchronization. 
We approached this problem in three steps.
Step 1 is to balance the communication load on each worker
through partitioning global model based on the word frequency.
Step 2 is to improve the speed with optimized collective communication.
Here we talk about Step 3: overlapping computation and communication 
in iteration execution.

In ``lda-rtt'',
we slice the global model partitions held on each worker into two sets. 
Slicing is done as follows:
we first sort the partition IDs in the ascending order.
And then we select the slice in turns: put the partition with the smallest ID value 
in the first slice, the second smallest partition in the second slice, 
then the third partition in the first slice, 
the fourth partition in the second slice and so on.
In this way, each slice will contain words with different frequencies 
from high to low.
We can view the whole process
as splitting one global data table distributed across workers
to two global data tables. 
During the sampling, 
when a worker finish processing the first slice, 
it uses another thread to rotate this slice.
At the same time, it continues processing the second slice.
Once the second slice is processed,
the first slice has been rotated and is ready for further processing.
When both slices have finished one round of rotation
and returned to the worker where they were before the sampling,
the sampling of one iteration is over.
the overlapping between computation and communication occurs 
when the worker processes one slice and rotate another slice at the same time.

In "lda-lgs",
we split the local data table into two slices.
During the sampling, 
each worker samples one slice 
and at the same time asks another thread to synchronize the other slice
through the paired ``syncLocalWithGlobal'' and ``syncGlobalWithLocal'' operations.
Because we use the result of partition ID modulo the number of slices
to select the slice ID for partitions,
local partitions with the same ID are guaranteed to be synchronized together.
%
\subsection{Concurrent Sampling}
% Data structure
In Harp-LDA, each worker can spawn numbers of threads to parallelize the local sampling.
Harp provides a computation component to control and reuse the multi-threading within one node.
For the local sampling algorithm, we use standard sparse LDA algorithm applied from
Mallet and Yahoo!LDA with additional modifications.
Different from their array based implementation, we use primitive integer
based hash maps from fastutil \cite{27} to manage each word's or document's topic count distribution.
We follow to Yahoo!LDA's implementation and allow all the threads sharing one copy of global model
rather than copying the global model to each threads as what Mallet does.

% Locking mechanism
As a result, locking mechanism is important when multiple threads read and write the same piece of data.
And in modern days, this problem becomes severe.
Earlier experiments of Yahoo!LDA paper only use 8 threads per machine and Petuum uses up to 16 threads.
but now under Intel Haswell architecture, one machine allows max to 72 threads parallelism.
It gives high challenge to the design of the locking mechanism.
Our implementation follows the idea from Yahoo!LDA and add other modifications.

Firstly, we add a read lock and write lock on the topic count map of each word.
Before sampling a word, a thread read out all the topic counts to local.
When it finishes sampling, it writes back the update to the local shared model data.
If the next word for sampling is the same word,
the sampling thread will use the thread local cache. 
The same words have already been grouped together during the preprocessing of the input data.
This can avoid repeating fetching topic counts from the shared local model.
Secondly, among all the write operations, 
we separate operations of "updating an existing topic entry" 
and operations of "adding a count to a new topic entry".
In operations of "updating an existing topic entry",
because the structure of map is not altered when updating the value of an existing key 
and reading a primitive integer is an atomic operation in modern x86 architecture,
it is safe to only use read lock on "update operations" 
and execute "read operation" and "update operation" concurrently. 
But "update operations" are still required to be mutually exclusive 
in order to ensure the correctness of the topic count values. 
In operation of "adding a count to a new topic entry", 
because the map structure is modified after adding a new entry,
we have to use write lock on "add operation".
With these two steps,
the concurrence of read and write operations in multi-threaded sampling is improved.
% Add a chart to describe the relations between all these operations
%
%
\section{Experiments}
\subsection{Experiment Settings}
We tested our "lda-rtt" and "lda-lgs" implementation
and compared with other two LDA implementation, Yahoo!LDA and Petuum LDA in the experiments.
Here both "lda-rtt" and Petuum LDA use model parallelism and data parallelism.
But "lda-rtt" uses synchronous collective communication with overlapping the computation and the communication
while Petuum LDA uses asynchronous "push" and "pull" operations to communicate between clients and servers.
Similarly, "lda-lgs" and Yahoo!LDA onlt uses data parallelism.
But "lda-lgs" uses optimized synchronous communication with overalapping the computation and the communication
while Yahoo!LDA uses asynchronous communication between clients and servers.
We compare these implementations and try to learn the reasons between the execution time
and the accuracy of the model trained. 

Two datasets are used. One is smaller, 
which includes 3,775,554 documents from Wikipedia, 1,107,903,672 tokens in total.
Another is bigger, which includes 50,546,979 documents from clueweb dataset, 12,359,079,355 tokens in total.
Vocabulary sizes on both datasets are 1 million. 
The number of topics is set to 10 thousand and alpha, beta are both set to 0.01.
 
The tests are done on Juliet cluster. 
Juliet cluster contains 32 nodes each of which contains 18 cores 72 threads
and 96 nodes each of which contains 24 cores 48 threads.
Nodes on Juliet are connect with two kinds of networks: 1 Gbps Ethernet and 16 Gbps Infiniband.
%
\subsection{Performance Comparison on Data Parallelism}


\subsection{Performance Comparison on Model Parallelism}
We compared our lda-rtt and lda-lgs performance with Yahoo!LDA and Petuum
on Juliet1 with 30 nodes, and Juliet2 with 120 nodes.

On enwiki dataset,


On clueweb dataset.

\subsection{Discussions}
Explain communication performance through logging and then explain results
through monitoring results.

%
%
\section{Related Work}
Much research work has focused on parallelizing the LDA algorithm.
Earlier work focus on using Collapsed Variational Bayes algorithm. 
Mahout LDA and Spark LDA both use this algorithm.
However, later research shows that 
this approach has high memory consumption \cite{12}
and slow convergence speed \cite{9}.

Later on, Collapsed Gibbs Sampling became a popular approach for parallelization. 
pLDA \cite{13} is an implementation based on CGS algorithm. pLDA has two versions, 
one based on MPI and another based on MapReduce.
The MPI version uses "allreduce" operation to synchronize the global model while
the MapReduce version uses "shuffle" operation to synchronize the global model.

Yahoo!LDA \cite{8} is probably the first well-known LDA implementation which 
uses client-server architecture with asynchronous communication
to synchronize local and global models.
The initial work was discussed in 
[An Architecture for Parallel Topic Models] and later improved in
[Scalable Inference in Latent Variable Models]
In Yahoo!LDA, Local models and the global model are distributed in the star model.  
local computation uses SparseLDA algorithm and applies optimized locking mechanisms
in multi-threading when accessing the shared local model.  
The synchronization between local and global model is done using a mechanism of
asynchronous delta aggregation.
Changes in the local model are sent to the server and aggregated.
Later the update global value is used to update the local value.
Each call to the server is non-blocking.
This kind of architecture design and data exchange mechanism provides a prototype and 
influenced later design in Parameter Server and Petuum.

GraphLab (now known as Dato \cite{5}) uses its PowerGraph framework \cite{18}
to implement LDA algorithm \cite{14}.
PowerGraph applies "gather-apply-scatter" (GAS) model to complete one iteration
of computation and communication. GAS model exploits the sparsity in document-word
relations. But its edge based computation pattern causes
the training data to be partitioned based on document-word pairs.
Because documents vertices are replicated on multiple machines,
not only do the topic counts of words have to be gathered and updated but also the
topic counts of documents.
This results in additional communication cost in model synchronization.
Additionally this implementation does not apply more efficient SparseLDA algorithm 

Peacock \cite{9} uses
a hierarchical distributed architecture to organize the LDA computation.
At the first layer, it uses SparseLDA algorithm with a lock-free parallel strategy 
by exploiting local model parallelism and data parallelism simultaneously. 
Partitioning based on the word frequency is also used to balance the compute load.
The design of this layer is close to "rotateGlobal" but still has two differences.
One is that here documents are sent to where the model locates but not rotate model
between documents. Another is that this design is for scheduling purposes
but does not consider routing optimization. 
The second layer also uses client-server architecture as what other tools do.
Yet again routing optimization is not included.

Parameter Server and Petuum (2014) both provide a framework to allow programming
machine learning algorithms in client-server architecture
with``push'' and ``pull'' operations.
Parameter Server put the global model in servers
and uses range based ``push'' and ``pull'' operations
to synchronize local and global models.
Range based operations allow workers to update a row
or a segment of parameters directly and provides a chance to batch
both the communication of updates and their processing on the parameter server. 
Parameter Server also focuses on the fault tolerance of the server management.
The local computation of its LDA implementation
using a combination of stochastic variational methods,
collapsed Gibbs sampling and distributed gradient descent. 

Petuum provides an operation called ``schedule'' other than ``push'' and ``pull'' operations.
with the scheduler component, Petuum allows model parallelism in addition to data parallelism.
In Petuum 1.1 LDA implementation, local compution also uses SparseLDA algorithm 
and similar to our ``lda-rtt'' implementation, each model partition 
are scheduled to rotate between workers. 
In \cite{21}, this kind of ``rotation'' is described as a scheduling strategy 
and the data communication still goes between clients and servers. 
But in the real code on GitHub, they directly send data between parallel workers.  
%
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}
%
% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
%
%
% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].
%
%
% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}
%
%
% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.
\section{Conclusion}
We proved that using synchronous communication methods perform better than asynchronous methods. 
% conference papers do not normally have an appendix
% use section* for acknowledgment
\section*{Acknowledgment}
We appreciate the system support offered by FutureGrid and Big Red II. We gratefully acknowledge support from National Science Foundation CAREER grant OCI-1149432.





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{99}
\setlength{\itemsep}{0pt}
\footnotesize
\bibitem{1}
D.M. Blei, A.Y. Ng, M.I. Jordan. ''Latent Dirichlet allocation''. Journal of Machine Learning Research, 2003.
\bibitem{2}
MPI Forum. ``MPI: A Message Passing Interface''. SC, 1993.
\bibitem{3}
\url{http://hadoop.apache.org}
\bibitem{4}
M. Zaharia et al. ``Spark: Cluster Computing with Working Sets''. HotCloud, 2010.
\bibitem{5}
\url{https://dato.com}
\bibitem{6}
M. Li et al. ''Scaling Distributed Machine Learning with the Parameter Server''. OSDI, 2014.
\bibitem{7}
E. Xing et al. ''Petuum: A New Platform for Distributed Machine Learning on Big Data''. KDD, 2015.
\bibitem{8}
A. Smola, S. Narayanamurthy. ''An Architecture for Parallel Topic Models''. VLDB, 2010.
\bibitem{9}
Y. Wang et al. ''Peacock: Learning Long-Tail Topic Features for Industrial Applications''.
ACM Transactions on Intelligent Systems and Technology 9 (4), 2014.
\bibitem{10}
\url{https://mahout.apache.org/users/clustering/latent-dirichlet-allocation.html}
\bibitem{11}
\url{http://spark.apache.org/docs/latest/mllib-clustering.html}
\bibitem{12}
D. Newman et al. ''Distributed Algorithms for Topic Models''. Journal of Machine Learning Research 10, 2009.
\bibitem{13}
Y. Wang et al. ''PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications''. AAIM, 2009.
\bibitem{14}
\url{https://github.com/dato-code/PowerGraph/blob/master/toolkits/topic_modeling/topic_modeling.dox}
\bibitem{15}
\url{https://github.com/petuum/bosen/wiki/Latent-Dirichlet-Allocation}
\bibitem{16}
P. Resnik, E. Hardisty. ''Gibbs Sampling for the Uninitiated''. Technical Report, 2010.
\bibitem{17}
L. Yao, D. Mimno, A. McCallum. 
''Efficient methods for topic model inference on streaming document collections''. KDD, 2009.
\bibitem{18}
J. Gonzalez et al. ``PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs''. OSDI, 2012.
\bibitem{19}
E. Chan, M. Heimlich, A. Purkayastha, and R. Geijn. ``Collective communication: theory, practice, and experience''. Concurrency and Computation: Practice and Experience 19 (13), 2007.
\bibitem{20}
Q. Ho et al. ''More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server''. NIPS, 2013.
\bibitem{21}
S. Lee et al. ''On Model Parallelization and Scheduling Strategies for Distributed Machine Learning''. NIPS, 2014.
\bibitem{22}
B. Zhang, Y. Ruan, J. Qiu. Harp: Collective Communication on Hadoop, IC2E, 2015.
\bibitem{23}
J. Dean and S. Ghemawat. ``Mapreduce: Simplified data processing on large clusters''. OSDI, 2004.
\bibitem{24}
Grzegorz Malewicz et al. ``Pregel: A System for Large-scale Graph Processing''. SIGMOD. 2010.
\bibitem{25}
Apache Giraph. https://giraph.apache.org/
\bibitem{26}
C.-T. Chu et al. ``Map-Reduce for Machine Learning on Multicore''. NIPS, 2006.
\bibitem{27}
\url{http://fastutil.di.unimi.it}
\end{thebibliography}




% that's all folks
\end{document}


