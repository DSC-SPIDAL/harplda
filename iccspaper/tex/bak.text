
%% bare_conf.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{multirow}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Harp-LDA}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Michael Shell}
% \IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
% Georgia Institute of Technology\\
% Atlanta, Georgia 30332--0250\\
% Email: http://www.michaelshell.org/contact.html}
% \and
% \IEEEauthorblockN{Homer Simpson}
% \IEEEauthorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
% \IEEEauthorblockA{Starfleet Academy\\
% San Francisco, California 96678--2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}
\author{\IEEEauthorblockN{Bingjing Zhang, Bo Peng, Judy Qiu}
	\IEEEauthorblockA{Computer Science Department\\
		Indiana University\\
		Bloomington, IN, USA\\
		zhangbj, pengb, xqiu@indiana.edu}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Nowadays when people build large-scale machine learning applications, they rely on many different styled distributed processing frameworks.
However people soon realize that choosing an appropriate tool is difficult.
The main reason is that among various synchronization strategies provided by these tools,  
though each research work suggests you use its own method, there is no clear answer which strategy is suitable for your data and application.
Contemporary tools such as Yahoo!LDA, Parameter Server or Petuum use client-server model with asynchronous communication methods to exchange local and global model data.
In this paper, we take Latent Dirichlet Allocation(LDA) as an example and show that with routing optimization, synchronous methods perform better than those asynchronous methods.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Nowadays many distributed machine learning frameworks commonly use client-server system model to parallelize applications.
In this model, they heavily use asynchronous communication methods to synchronize local learning model data at the client side and the global learning model at the server side.
Yahoo!LDA, Parameter Server and Petuum all are examples of such parallelism pattern. The advantages of this pattern promoted by these research works are:
1) Avoid the overhead of global synchronization
2) Due to the sparsity of the model data, the local model data is small and te related communication is low. 
3) 
The advantages of using synchronous communication methods.
1) better partitioning
2) routing optimization 
History of Harp and collective communications.

parameter server uses 10 billion parameters (5m*2k model, 5 billion tokens)
Petuum LDA ClueWeb10 \% 10B tokens 50M webpages, 160K words, 1K topics α = β = 0.1.
Peacock The vocabulary size of SOSO is around $2.1*10^5$. topic from 1k to 10k
Problems and contributions.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
\section{Related Work}
pLDA
Yahoo!LDA
Petuum LDA
Parameter Server
\section{LDA Algorithm}
Discussions of LDA algorithm and related features of the algorithm.
How it affects communication.

\section{Cost Model}

\subsection{LDA model}

Latent Dirichlet Allocation(LDA) is a generative probabilistic data modeling technique. Input data is document collection, where each document is a bag of words. LDA model the observed data by introducing some latent variables, calls topics. Topic is in fact a distribution of words, which try to capture the underline semantic connections and structures inside the observed data. In LDA model, a document is a mixture over latent topics and each topic is a multinomial distribution over words. In the generative process, for document $j$, we first draw a topic distribution $\theta_{j}$ from a Dirichlet with a hyper-parameter $\alpha$, then for each word $i$ in this document, we draw a topic $z_{ij} = k$ from the multinomial distribution $\theta_{j}$. Finally, word $x_{ij}$ is drawn from a multinomial $\phi_{wk\mid k= z_{ij} }$, which is also drawn from a Dirichlet with parameter $\beta$. This process is often denoted by graphic models as figure 1. Here, the words $x_{ij}$ are observed variables, $\theta$, $\phi$, $z$ are latent variables, $\alpha$ and $\beta$ are hyper parameters. 

The task of LDA inference is to compute the posterior distribution of the latent variables given the observed variables. There are many approximate inference algorithms[][][]. In practice on large data, Collapsed Gibbs Sampling(CGS) is the one proves it’s high scalability. Collapse is a procedure to integrate out $\theta$,$\phi$ and just sample the latent variables $z$. Gibbs Sampling is one kind of Markov chain Monte Carlo algorithm for inference. There are three phases: initialization, burn-in and stationary. 

In initialization, each word is initialized by a random topic denoted as $z_{ij}$. Then it begin to reassign topic to each word $w_{ij}$ according the conditional probability of $z_{ij}$, which calls sampling.

\begin{align}
p\left ( z_{ij}=k\mid z^{\neg ij},x,\alpha,\beta \right )\propto \frac{N_{wk}^{\neg ij} + \beta} {\sum_{w}N_{wk}^{\neg ij} + V\beta}\left ( N_{kj}^{\neg ij} + \alpha \right )
\end{align}

Here , superscript $\neg ij$ means that the corresponding word is excluded in the counts. $V$ is vocabulary size. $N_{wk}$ is the count of word w assigned to topic $k$, $N_{kj}$ is the count of topic assigned in document $j$, which are sufficient statistics for the latent variable $\theta$ and $\phi$. The latent variables can be represented by three matrix $Z_{ij}$, $N_{wk}$ and $N_{kj}$. 

$\alpha$ and $\beta$ are also called concentration parameters, which controls the topics density in the final model. The larger the $\alpha$ and $\beta$, the more topics can be drawn into a document and assigned to a word, and more non-zero cells in each row of the $N_{wk}$ and $N_{kj}$ matrix. Although a useful LDA trainer often has the feature of $\alpha$ and $\beta$ optimization to dynamically tuned to fit the input data, in this paper, we skip this feature and fix both $\alpha$ and $\beta$ to a common used value 0.01 to exclude the effects on performance caused by their dynamics. Then the final model data size has a fixed ratio to it’s initialized state.

Latent variables will gradually converge in the processes of iterative sampling. This is the phase of burn in, and finally reach the stationary state. In stationary state, we can draw samples from the sampling process and use those samples to calculate the posterior distribution.

Sampling on $z_{ij}$ in CGS is a strictly sequential process. AD-LDA[] is the seminal work relaxing the sequential sampling requirement . It assumes that the dependence between one topic assignment $z_{ij}$ and another $z_{i{}’j{}’}$ is weak in case different words in different documents are sampled concurrently. In AD-LDA, input data are partitioned into n subsets, n Gibbs Samplers running parallel on each collection, and each sampler synchronizes it's model data with others at some time points. This parallel version still learns a useful model. This established the foundation of large scale parallel implementations of CGS of LDA trainers on large scale data in practice. 

\subsection{ Cost model parameters}

This paper focus on the performance of parallel LDA trainers on large data and large model. There are three main relevant aspects, which are sampling algorithm, parallel strategy, input data properties, and cluster configuration.


* Sampling Algorithm

Sampling algorithm determines the computation complexity. SparseLDA[] is an optimized CGS sampling algorithm used in the state-of-the-art LDA trainers. It decomposes the equation (1) into three parts:

\begin{align}
p\left ( z_{ij}=k\mid z^{\neg ij},x,\alpha,\beta \right )\propto \frac{N_{wk}^{\neg ij}(N_{kj}^{\neg ij}+\alpha) + \beta*N_{kj}^{\neg ij} + \alpha\beta} {\sum_{w}N_{wk}^{\neg ij} + V\beta}
\end{align}

The denominator is a constant in the process of one word sampling. The third part of numerator is also a constant, the second part is non-zero only when $N_{kj}$ is non-zero, and the first part is non-zero only when $N_{wk}$ is non-zero. In naive CGS sampling, the conditional probability will compute $K$ times, while in SparseLDA, the computation can be decreased to non-zero items number in $N_{wk}$ and $N_{kj}$, which are much smaller than $K$ on average. We found that in practice, the sampling performance is more memory bounded than computation bounded, for the computation is very simple and memory access to two large matrix is not easy to be cache friendly.

And further, CGS has a feature of exchangeability that the order of word sampling can be changed. In practice, sampling can take the order on row or column on the document-word matrix. Equation(2) is the form optimized for row order, called sample-by-doc. In this case, $N_{kj}$ can be cached for the words in the same row, and the computation complexity in terms of random memory access times is $\mathcal{O}(\#\{N_{wk}<>0\})$. Symmetrically, sample-by-word will have the complexity of $\mathcal{O}(\#\{N_{kj}<>0\})$. 

* Parallelism Strategy
Input data is actually a document-word matrix, data partition can be done either in the rows or in the columns. If data are partitioned by rows, each subset data has it’s local $z$, $N_{kj}$, $N_{wk}$ model data and only $N_{wk}$ need to be synchronized with others. In general applications, the row number is much larger than column number, so partition by rows will get smaller model data size, here we only call the shared word-topic matrix as model data. 

Model data parallelism, balabala....

There are many possible communication strategies which control how to do model data synchronization among parallel units. Modern cluster has two level of parallel units, one is distributed processes on inter-nodes level and another is multi-threading inside node level. In this paper, we focus on the inter-nodes level by exploring the differences among the communication strategies.

* Data 
Input data can be characterized by total numbers of tokens, denoted as $W$, and number of documents, denoted as $D$. The model data $N_{wk}$ is a $V*K$ matrix and $N_{kj}$ is a $D*K$ matrix, where $V$ is the vocabulary size and $K$ is the topic number. 

* cluster configuration
Cluster configurations include nodes number $N$ and networking bandwidth $B$, memory size $M$ for each node, and threads number $T$ for each node. As manycore technology brings more powerful machines for complicated computation applications, large scale machine learning applications will definitely benefit from it. Relatively small number of N with large number of T can reach high scale of parallelism, which is more like a traditional HPC cluster other than a cloud cluster.

\subsection{Cost model}

LDA is an iterative algorithm, it keeps sampling on the input data, updating(synchronizing) the model data, until converges. In the computation part, one iteration is one pass on sampling the input data. In the synchronization part, one iteration is one pass to synchronize all the model data. As we described above, both parts are highly related to the model data size, not the matrix size but the non-zero items count. 

%1. First, we try to find out the relationship between input data W and model data V, K.  \\

Power law distribution is a general phenomenon. It has another equal form for text data as Zipf's law, where the frequency of a word is proportional to the reciprocal of it's rank.
\begin{align}
freq(i) = C*i^{-\lambda}
\end{align}
In equation(3), i is word rank, and $\lambda$ is near 1.

There are total V unique words in the input data, then we have:  
\begin{align}
W &= \sum_{i=1}^{V}(freq(i))) = \sum_{i=1}^{V}(C*i^{-\lambda})  \nonumber\\
   &\approx C*(ln(V) + \gamma + \frac{1}{2V})
\end{align}
If $\lambda$ is 1, this is the partial sum of harmonic series which have logarithmic growth, where $\gamma$ is the Euler–Mascheroni constant $\approx$ 0.57721.

Model data is a very huge but sparse V*K matrix. In general setting, V is 1M, K is 1K, while for big model it can even reach 1M*1M. The non-zero count of model data is the true model size, denoted as S, $S << V*K$. 

In the initialization phrase of CGS, word-topic count matrix is initialized by random topic assignment for each work. So the word i will get max(K, freq(i)) non-zero cells. 
if $freq(J) = K$, $J = C/K$, we get:
\begin{align}
S &= \sum_{i=1}^{J}K + \sum_{i=J+1}^{V}freq(i) = W - \sum_{i=1}^{J}freq(i) + \sum_{i=1}^{J}K  \nonumber\\
  &= C*(lnV + lnK- lnC +1)
\end{align}

The true model size S is logarithmic to matrix size V*K. This does not mean S is small, for the constant $C = freq(1)$ can be very large, even $C*lnV*K$ can be huge. But it really means that the increase of dimension of the model will not increase the model data size dramatically. 

With the progress of iterations and algorithm converges, the model data size will shrink. The concentration parameters $\alpha$ and $\beta$ control the final sparsity of the topic distribution. When stationary state reached, the average count value will drop to a certain small ratio of K.
\begin{align}
S=mean(word-topic count)*V = \delta*K*V
\end{align}


% Second, we try to find out how the model data is distributed among the nodes. \\

After input data is partitioned by documents to each node of the cluster, a local model data S' will be built up and used in local computation, and this local model data should synchronize with global model data S frequently to make the training process converge. In fact, the synchronization frequency is highly relevant to the final model accuracy.

This data partition strategy can decrease local input data W' linear to node number N, we get $W{}' = W / N$

For computation is proportional to the total word number W', this strategy is friendly to computation, and the more nodes the better performance we can expect. 

And, if $C' = C/N$ assumed, the actual local model size S' in initialization is:
\begin{align}
S' &= C' * (lnV' + lnK -lnC' +1)    \nonumber\\
   &\approx \frac{C}{N}(lnV +lnK -lnC + 1 +lnN)   \nonumber\\
   &\approx \frac{S}{N} + \frac{C}{N}lnN
\end{align}

In general configurations $lnN$ is smaller than $lnV+lnK-lnC+1$, so local model size S' is no more than $\frac{2}{N}S$, the initialized local model data size is controllable by data partition. 

But, when model data synchronization begins, all words in local vocabulary need to fetch the corresponding global model data. The local vocabulary size V' will then determinate the both communication data volume and local model size in burn-in phase, there comes the problem.

$W' \approx C'*(ln(V') + \gamma + \frac{1}{2V'})$

where $C' \approx C/N, W' = W/N$

we can get:

$V' \approx V$

This is also easy to understand that when documents partitioned to N nodes, every words with frequency larger than N will get a high probability occurring on each node. 

assume at rank L, $freq(L) = N$, we get:
$L = \frac{W}{ (lnV + \gamma) * N}$

e.g. enwiki-3.7M dataset,W=1B, V=1M, N=100, we get $L = 0.69V$.  clueweb-10B dataset, W = 10B, V=1M, N = 100, $L > V$. For reasonable large input data set, L should be easily larger than V, which means it will receive and hold almost all the global model data locally.

So, we get a conclusion that because of the power law distribution of data exists, general data parallelism can help to distribute input data among nodes and parallel the computation tasks accordingly, but it can not effectively distribute model data among nodes. Even worse for big data, when more nodes introduced, model data synchronization will finally become the bottleneck.




\section{Synchronous Communication Methods in Harp-LDA}
Drawback of traditional collective communication methods.
\section{Harp-LDA Implementation}
\section{Experiments}


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}
We proved that using synchronous communication methods perform better than asynchronous methods. 
% conference papers do not normally have an appendix


% use section* for acknowledgment
\section*{Acknowledgment}
We appreciate the system support offered by FutureGrid and Big Red II. We gratefully acknowledge support from National Science Foundation CAREER grant OCI-1149432.





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{99}
\setlength{\itemsep}{0pt}
\footnotesize
\bibitem{1}
J. Dean and S. Ghemawat. ``Mapreduce: Simplified data processing on large clusters''. OSDI, 2004.
\bibitem{2}
Apache Hadoop. http://hadoop.apache.org
\bibitem{3}
J. Ekanayake et al. ``Twister: A Runtime for iterative MapReduce.” Workshop on MapReduce and its Applications, HPDC, 2010.
\bibitem{4}
Y. Bu, B. Howe, M. Balazinska, and M. Ernst. ``Haloop: Efficient Iterative Data Processing on Large Clusters''. VLDB, 2010.
\bibitem{5}
M. Zaharia et al. ``Spark: Cluster Computing with Working Sets''. HotCloud, 2010.
\bibitem{6}
Grzegorz Malewicz et al. ``Pregel: A System for Large-scale Graph Processing''. SIGMOD. 2010.
\bibitem{7}
Apache Giraph. https://giraph.apache.org/
%\bibitem{8}
%Apache Hama. https://hama.apache.org/
\bibitem{9}
S. Lloyd. ``Least Squares Quantization in PCM''. IEEE Transactions on Information Theory 28 (2), 1982.
\bibitem{10}
Apache Mahout. https://mahout.apache.org/
\bibitem{11}
J. Qiu, B. Zhang. ``Mammoth Data in the Cloud: Clustering Social Images''. In Clouds, Grids and Big Data, IOS Press, 2013.
\bibitem{12}
B. Zhang, J. Qiu. ``High Performance Clustering of Social Images in a Map-Collective Programming Model”. Poster in SoCC, 2013.
\bibitem{13}
E. Chan, M. Heimlich, A. Purkayastha, and R. Geijn. ``Collective communication: theory, practice, and experience''. Concurrency and Computation: Practice and Experience 19 (13), 2007.
\bibitem{14}
C.-T. Chu et al. ``Map-Reduce for Machine Learning on Multicore''. NIPS, 2006.
\bibitem{15}
Harp. http://salsaproj.indiana.edu/harp/index.html
\bibitem{16}
Y. Low et al. ``Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud''. PVLDB, 2012.
\bibitem{17}
J. Gonzalez et al. ``PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs''. OSDI, 2012.
\bibitem{18}
R. Xin et al. ``GraphX: A Resilient Distributed Graph System on Spark''. GRADES, SIGMOD workshop, 2013.
\bibitem{19}
M. Chowdhury et al. ``Managing Data Transfers in Computer Clusters with Orchestra''. SIGCOM, 2011.
\bibitem{20}
T. Gunarathne, J. Qiu and D. Gannon. ``Towards a Collective Layer in the Big Data Stack''. CCGrid, 2014.
\bibitem{21}
T. Fruchterman, M. Reingold. ``Graph Drawing by Force-Directed Placement'', Software – Practice \& Experience 21 (11), 1991.
\bibitem{22}
Y. Ruan et al. ``A Robust and Scalable Solution for Interpolative Multidimensional Scaling With Weighting''. E-Science, 2013.
\bibitem{23}
MPI Forum. ``MPI: A Message Passing Interface''. SC, 1993.
\bibitem{24}
Big Red II. https://kb.iu.edu/data/bcqt.html
\bibitem{25}
G. Fox. ``Robust Scalable Visualized Clustering in Vector and non Vector Semimetric Spaces''. Parallel Processing Letters 23, 2013.
\bibitem{26}
X. Gao and J. Qiu. ``Social Media Data Analysis with IndexedHBase and Iterative MapReduce''. Workshop on Many-Task Computing on Clouds, Grids, and Supercomputers (MTAGS), SC, 2013.
\bibitem{27}
Y. Ruan et al. ``Integration of Clustering and Multidimensional Scaling to Determine Phylogenetic Trees as Spherical Phylograms Visualized in 3 Dimensions''. C4Bio, CCGrid worksop, 2014.
\end{thebibliography}




% that's all folks
\end{document}


